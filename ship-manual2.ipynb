{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict unknown values using one-hot vectors\n",
    "## Data prepartion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/m0c_logistic_regression.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/titanic.csv\n",
    "\n",
    "# Open our dataset from file\n",
    "dataset = pandas.read_csv(\"titanic.csv\", index_col=False, sep=\",\", header=0)\n",
    "\n",
    "# Fill missing cabin information with 'Unknown'\n",
    "dataset[\"Cabin\"].fillna(\"Unknown\", inplace=True)\n",
    "\n",
    "# Remove rows missing Age information\n",
    "dataset.dropna(subset=[\"Age\"], inplace=True)\n",
    "\n",
    "# Remove the Name, PassengerId, and Ticket fields\n",
    "# This is optional; it makes it easier to read our print-outs\n",
    "dataset.drop([\"PassengerId\", \"Name\", \"Ticket\"], axis=1, inplace=True)\n",
    "\n",
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About Our Model\n",
    "We'll training a model type known as Logistic Regression, which will predict who survives the Titanic disaster.\n",
    "\n",
    "Numerical Only\n",
    "Let's create a model that uses only the numerical features.\n",
    "\n",
    "First, we'll use Pclass here as an ordinal feature, rather than a one-hot encoded categorical feature.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from m0c_logistic_regression import train_logistic_regression\n",
    "\n",
    "features = [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\"] \n",
    "loss_numerical_only = train_logistic_regression(dataset, features)\n",
    "\n",
    "print(f\"Numerical-Only, Log-Loss (cost): {loss_numerical_only}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Categorical Features\n",
    "Categorical features with only two potential values can be encoded in a single column, as 0 or 1.\n",
    "\n",
    "We'll convert Sex values into IsFemale - a 0 for male and 1 for female - and include that in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap male / female with numerical values\n",
    "# We can do this because there are only two categories\n",
    "dataset[\"IsFemale\"] = dataset.Sex.replace({'male':0, 'female':1})\n",
    "\n",
    "# Print out the first few rows of the dataset\n",
    "print(dataset.head())\n",
    "\n",
    "# Run and test the model, also using IsFemale this time\n",
    "features = [\"Age\", \"Pclass\", \"SibSp\", \"Parch\", \"Fare\", \"IsFemale\"] \n",
    "loss_binary_categoricals = train_logistic_regression(dataset, features)\n",
    "\n",
    "print(f\"\\nNumerical + Sex, Log-Loss (cost): {loss_binary_categoricals}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One-Hot Encoding\n",
    "Let's convert Pclass into a categorical feature using one-hot encoding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Possible values for PClass: {dataset['Pclass'].unique()}\")\n",
    "\n",
    "# Use Pandas to One-Hot encode the PClass category\n",
    "dataset_with_one_hot = pandas.get_dummies(dataset, columns=[\"Pclass\"], drop_first=False)\n",
    "\n",
    "# Add back in the old Pclass column, for learning purposes\n",
    "dataset_with_one_hot[\"Pclass\"] = dataset.Pclass\n",
    "\n",
    "# Print out the first few rows\n",
    "dataset_with_one_hot.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Pclass converted into three values: Pclass_1, Pclass_2 and Pclass_3.\n",
    "\n",
    "Rows with Pclass values of 1 have a value in the Pclass_1 column. We see the same pattern for values of 2 and 3.\n",
    "\n",
    "Now, re-run the model, and treat Pclass values as a categorical values, rather than ordinal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and test the model, also using Pclass as a categorical feature this time\n",
    "features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"IsFemale\",\n",
    "            \"Pclass_1\", \"Pclass_2\", \"Pclass_3\"]\n",
    "\n",
    "loss_pclass_categorical = train_logistic_regression(dataset_with_one_hot, features)\n",
    "\n",
    "print(f\"\\nNumerical, Sex, Categorical Pclass, Log-Loss (cost): {loss_pclass_categorical}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that many passengers had Cabin information. Cabin is a categorical feature and should be a good predictor of survival, because people in lower cabins probably had little time to escape during the sinking.\n",
    "\n",
    "Let's encode cabin using one-hot vectors, and include it in a model. This time, there are so many cabins that we won't print them all out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas to One-Hot encode the Cabin and Pclass categories\n",
    "dataset_with_one_hot = pandas.get_dummies(dataset, columns=[\"Pclass\", \"Cabin\"], drop_first=False)\n",
    "\n",
    "# Find cabin column names\n",
    "cabin_column_names = list(c for c in dataset_with_one_hot.columns if c.startswith(\"Cabin_\"))\n",
    "\n",
    "# Print out how many cabins there were\n",
    "print(len(cabin_column_names), \"cabins found\")\n",
    "\n",
    "# Make a list of features\n",
    "features = [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"IsFemale\",\n",
    "            \"Pclass_1\", \"Pclass_2\", \"Pclass_3\"] + \\\n",
    "            cabin_column_names\n",
    "\n",
    "# Run the model and print the result\n",
    "loss_cabin_categorical = train_logistic_regression(dataset_with_one_hot, features)\n",
    "\n",
    "print(f\"\\nNumerical, Sex, Categorical Pclass, Cabin, Log-Loss (cost): {loss_cabin_categorical}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improving Power\n",
    "Including very large numbers of categorical classes - for example, 135 cabins - is often not the best way to train a model, because the model only has a few examples of each category class to learn from.\n",
    "\n",
    "Sometimes, we can improve models if we simplify features. Cabin was probably useful because it indicated which Titanic deck people were probably situated in: those in lower decks would have had their quarters flooded first.\n",
    "\n",
    "It might become simpler to use deck information, instead of categorizing people into Cabins.\n",
    "\n",
    "Let's simplify what we have run, replacing the 135 Cabin categories with a simpler Deck category that has only 9 values: A - G, T, and U (Unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have cabin names, like A31, G45. The letter refers to the deck that\n",
    "# the cabin was on. Extract just the deck and save it to a column. \n",
    "dataset[\"Deck\"] = [c[0] for c in dataset.Cabin]\n",
    "\n",
    "print(\"Decks: \", sorted(dataset.Deck.unique()))\n",
    "\n",
    "# Create one-hot vectors for:\n",
    "# Pclass - the class of ticket. (This could be treated as ordinal or categorical)\n",
    "# Deck - the deck that the cabin was on\n",
    "dataset_with_one_hot = pandas.get_dummies(dataset, columns=[\"Pclass\", \"Deck\"], drop_first=False)\n",
    "\n",
    "# Find the deck names\n",
    "deck_of_cabin_column_names = list(c for c in dataset_with_one_hot.columns if c.startswith(\"Deck_\"))\n",
    " \n",
    "features = [\"Age\", \"IsFemale\", \"SibSp\", \"Parch\", \"Fare\", \n",
    "            \"Pclass_1\", \"Pclass_2\", \"Pclass_3\",\n",
    "            \"Deck_A\", \"Deck_B\", \"Deck_C\", \"Deck_D\", \n",
    "            \"Deck_E\", \"Deck_F\", \"Deck_G\", \"Deck_U\", \"Deck_T\"]\n",
    "\n",
    "loss_deck = train_logistic_regression(dataset_with_one_hot, features)\n",
    "\n",
    "print(f\"\\nSimplifying Cabin Into Deck, Log-Loss (cost): {loss_deck}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Comparing Model loss for these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dataframe to create a comparison table of metrics\n",
    "# Copy metrics from previous Unit\n",
    "\n",
    "l =[[\"Numeric Features Only\", loss_numerical_only],\n",
    "    [\"Adding Sex as Binary\", loss_binary_categoricals],\n",
    "    [\"Treating Pclass as Categorical\", loss_pclass_categorical],\n",
    "    [\"Using Cabin as Categorical\", loss_cabin_categorical],\n",
    "    [\"Using Deck rather than Cabin\", loss_deck]]\n",
    "\n",
    "pandas.DataFrame(l, columns=[\"Dataset\", \"Log-Loss (Low is better)\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
